services:
  frontend:
    #bouw de frontend container met de dockerfile in de map front-end
    build: ./front-end
    #koppel poort 5173 van de container aan poort 5173 op je computer
    ports:
      - "5173:5173"
    #synchroniseer de lokale map met de container zodat wijzigingen direct zichtbaar zijn
    #de tweede regel zorgt dat node_modules binnen de container blijft en niet lokaal
    volumes:
      - ./front-end:/app
      - /app/node_modules
    #verbind de frontend met hetzelfde netwerk als de backend
    networks:
      - app-network
  backend:
    #bouw de backend container met de dockerfile in de map back-end
    build: ./back-end
    #koppel poort 5000 in de container aan poort 5050 op je computer
    ports:
      - "5050:5000"
    #synchroniseer de lokale map met de container zodat je code direct updatet
    volumes:
      - ./back-end:/app
    #verbind de backend met hetzelfde netwerk als de frontend
    networks:
      - app-network
    depends_on:
      - mongo
      - ollama

  mongo:
    #mongodb database service voor users (login/register)
    image: mongo:7
    restart: always
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db
    networks:
      - app-network

  ollama:
    #ollama service for LLM analysis
    image: ollama/ollama:latest
    restart: always
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - app-network
    #pull llama3 model on startup
    entrypoint: ["sh", "-c", "ollama serve & sleep 10 && ollama pull llama3 && wait"]
    environment:
      # Limit Ollama parallelism and threads to reduce CPU usage
      - OLLAMA_NUM_PARALLEL=1 # Limit parallel requests
      - OLLAMA_MAX_LOADED_MODELS=1 # Keep only 1 model loaded
      - OLLAMA_NUM_THREAD=3 # Limit CPU threads (default is 4+)
      - OLLAMA_MAX_BATCH_SIZE=512 # Reduce batch size
      - CUDA_VISIBLE_DEVICES= # Disable GPU (if present)
    #CPU resource limits to prevent overheating
    cpus: "3" # Limit to 2.5 CPU cores max (Docker Compose format)
    # memory: 4g    # Optional: add memory limit if needed

#definieer het netwerk waar beide containers in zitten
networks:
  app-network:
    #bridge betekent dat docker een intern netwerk aanmaakt waar containers met elkaar kunnen praten
    driver: bridge

volumes:
  mongo-data:
    #volume om de mongodb data te bewaren
  ollama-data:
    #volume om de ollama models te bewaren
